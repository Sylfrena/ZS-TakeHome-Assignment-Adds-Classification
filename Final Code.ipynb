{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#D81B60\"> Classifying YouTube Comments for Advertisement </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#D81B60\"> Customized Utility Functions </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_group(day):\n",
    "    if(day<5):\n",
    "        return \"weekday\"\n",
    "    elif(day>4 and day <7):\n",
    "        return \"weekend\"\n",
    "    else:\n",
    "        return \"unknown_day\"\n",
    "        \n",
    "def time_group(time):\n",
    "    if(time==99):\n",
    "        return \"unknown_time\"\n",
    "    elif(time in range(17,21)):\n",
    "        return \"peak\"\n",
    "    else:\n",
    "        return \"normal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(content):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    content = re.sub('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+','', content)               # remove urls \n",
    "    content = re.sub(\"<[^>]*>\",\" \",content)                                               # remove html tags \n",
    "    content = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", content)                             # remove Numbers \n",
    "    content = re.sub(r'[^\\w\\s]',' ',content.lower())                                      # remove alphan-numeric characters\n",
    "    c_list=content.split(\" \")\n",
    "    new_list=[]\n",
    "    for i in range(len(c_list)):\n",
    "        if(len(c_list[i])>2):\n",
    "            word=c_list[i]\n",
    "            word=stemmer.stem(word)\n",
    "            new_list.append(lemm.lemmatize(word))\n",
    "    content=\" \".join(new_list)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_presence(content):\n",
    "    urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', content)\n",
    "    presence=0\n",
    "    if(\"bit.ly\" in content):\n",
    "        presence=1\n",
    "    if(\"moneygq.com\" in content.lower()):\n",
    "        presence=1    \n",
    "    if(\"zonepa.com\" in content.lower()):\n",
    "        presence=1        \n",
    "    if(len(urls)>0 and (('http://www.youtube.com' not in urls) and ('https://www.youtube.com' not in urls) and \\\n",
    "                      ('http://youtu.be' not in urls) and ('https://youtu.be' not in urls))):\n",
    "    #     print(urls)\n",
    "        presence=1\n",
    "    return presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_zero_var(df,threshold=0.0):\n",
    "    cols=df.columns\n",
    "    near_zero_vars=[]\n",
    "    for col in cols:\n",
    "        if(np.var(df[col])<=threshold): # variable has zero variance and doesn't affect the target\n",
    "            near_zero_vars.append(col)\n",
    "    \n",
    "    return(list(set(cols)-set(near_zero_vars)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlation(df, thresh=0.9):\n",
    "    \"\"\"\n",
    "    Given a numeric pd.DataFrame, this will find highly correlated features,\n",
    "    and return a list of features to remove\n",
    "    params:\n",
    "    - df : pd.DataFrame\n",
    "    - thresh : correlation threshold, will remove one of pairs of features with\n",
    "               a correlation greater than this value\n",
    "    \"\"\"\n",
    "    \n",
    "    corrMatrix = df.corr()\n",
    "    corrMatrix.loc[:,:] =  np.tril(corrMatrix, k=-1)\n",
    "\n",
    "    already_in = set()\n",
    "    result = []\n",
    "\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] > thresh].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            result.append(perfect_corr)\n",
    "\n",
    "\n",
    "    select_nested = [f[1:] for f in result]\n",
    "    select_flat = [i for j in select_nested for i in j]\n",
    "    return select_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_reduction(df,true_labels,threshold=0.0):\n",
    "    res_features=[]\n",
    "    for col in df.columns:\n",
    "        if(auc(df[col],true_labels,reorder=True)>=threshold):\n",
    "            res_features.append(col)\n",
    "    return res_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model,model_name=\"random_forest\"):\n",
    "    print(\"Training Model : ######################## \",model_name,\" ############################### \")\n",
    "    model.fit(train_x,train_y)\n",
    "    train_pred=model.predict(train_x)\n",
    "    \n",
    "    print(\"Validate Model : ######################## \",model_name,\" ############################### \")\n",
    "    valid_pred=model.predict(valid_x)\n",
    "    print()\n",
    "    \n",
    "    print(\"Training Error : \")\n",
    "    print(roc_auc_score(train_y,train_pred))\n",
    "    print()\n",
    "    \n",
    "    print(\"Validation Error : \")\n",
    "    print(roc_auc_score(valid_y,valid_pred))\n",
    "    print()\n",
    "    \n",
    "    print(\"Training Confusion Matrix :\")\n",
    "    print(confusion_matrix(train_y,train_pred))\n",
    "    print()\n",
    "    \n",
    "    print(\"Validation Confusion Matrix :\")\n",
    "    print(confusion_matrix(valid_y,valid_pred))\n",
    "    print()\n",
    "    \n",
    "#     print(\"Test Predictions :\")\n",
    "    if(model_name==\"vooting_classifier\"):\n",
    "        pred_test=model.predict(final_test_df)\n",
    "        response=pd.DataFrame({\n",
    "            \"ID\":test_ids,\n",
    "            \"CLASS\":pred_test\n",
    "        })[[\"ID\",\"CLASS\"]]\n",
    "        response.to_csv(\"./result/\"+model_name+\"_result.csv\",index=False)\n",
    "        return model\n",
    "    else:\n",
    "#         pred_test=model.predict_proba(final_test_df)\n",
    "        pred_test=model.predict(final_test_df)\n",
    "        response=pd.DataFrame({\n",
    "            \"ID\":test_ids,\n",
    "#             \"CLASS\":pred_test[:,1]\n",
    "            \"CLASS\":pred_test\n",
    "        })[[\"ID\",\"CLASS\"]]\n",
    "        response.to_csv(\"./result/\"+model_name+\"_result.csv\",index=False)\n",
    "    \n",
    "    \n",
    "    features=final_train_df.columns\n",
    "#     features.extend(['url_presence','len_review'])\n",
    "    \n",
    "    if((model_name!=\"logistic_regresssion\") and (model_name!=\"SVC\")):\n",
    "        feature_importance=pd.DataFrame({\n",
    "            \"features\":features,\n",
    "            \"importance\":model.feature_importances_*100\n",
    "        })[[\"features\",\"importance\"]]\n",
    "        feature_importance.to_csv(\"./result/\"+model_name+\"_importance.csv\",index=False)\n",
    "        print(\"Top 10 features from %s model \" %model_name)\n",
    "        print(feature_importance[feature_importance[\"importance\"]>0].sort_values(\"importance\",ascending=False).head(10))\n",
    "    else:\n",
    "        logistic_regression_coefficient=pd.DataFrame({\n",
    "            \"features\":features,\n",
    "            \"coefficient\":model.coef_[0][:]\n",
    "        })[[\"features\",\"coefficient\"]]\n",
    "        print(\"Regression Coefficient from %s model \" %model_name)\n",
    "        print(logistic_regression_coefficient)\n",
    "#         print(model.coef_[0][:])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#F57C00\"> Load Data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size : 1157\n",
      "Test data size : 799\n"
     ]
    }
   ],
   "source": [
    "data_dir=\"./Data Sets/\"\n",
    "train_data=pd.read_csv(data_dir+\"/train.csv\")\n",
    "test_data=pd.read_csv(data_dir+\"/test.csv\")\n",
    "print(\"Train data size :\",train_data.shape[0])\n",
    "print(\"Test data size :\",test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids=test_data.ID\n",
    "test_data.drop(\"ID\",axis=1,inplace=True)\n",
    "true_labels=train_data.CLASS\n",
    "train_data.drop(\"CLASS\",axis=1,inplace=True)\n",
    "combine_data=train_data.append(test_data,ignore_index=True)\n",
    "combine_data.drop(\"COMMENT_ID\",inplace=True,axis=1)\n",
    "combine_data.DATE=pd.to_datetime(combine_data.DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07 06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09 08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10 16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ferleck ferles</td>\n",
       "      <td>2013-11-27 21:39:24</td>\n",
       "      <td>Subscribe to my channel ﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>2013-11-28 16:30:13</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AUTHOR                DATE  \\\n",
       "0        Julius NM 2013-11-07 06:20:48   \n",
       "1  ElNino Melendez 2013-11-09 08:28:43   \n",
       "2           GsMega 2013-11-10 16:05:38   \n",
       "3   ferleck ferles 2013-11-27 21:39:24   \n",
       "4      BeBe Burkey 2013-11-28 16:30:13   \n",
       "\n",
       "                                             CONTENT  \n",
       "0  Huh, anyway check out this you[tube] channel: ...  \n",
       "1   me shaking my sexy ass on my channel enjoy ^_^ ﻿  \n",
       "2            watch?v=vtaRGgvGtWQ   Check this out .﻿  \n",
       "3                          Subscribe to my channel ﻿  \n",
       "4  and u should.d check my channel and tell me wh...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#F57C00\"> Summary of Null Values present in the Data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of null values present in the Train Data : \n",
      "\n",
      "COMMENT_ID     0.000000\n",
      "AUTHOR         0.000000\n",
      "DATE          11.927398\n",
      "CONTENT        0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of null values present in the Train Data : \\n\")\n",
    "print((train_data.isnull().sum()/train_data.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of null values present in the Test Data : \n",
      "\n",
      "COMMENT_ID     0.00000\n",
      "AUTHOR         0.00000\n",
      "DATE          13.39174\n",
      "CONTENT        0.00000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of null values present in the Test Data : \\n\")\n",
    "print((test_data.isnull().sum()/test_data.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Adds : 586 and Comments : 571 \n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of Adds : %s and Comments : %s \" %tuple(true_labels.value_counts().values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#D81B60\"> Feature Engineering  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#F57C00\"> Create Time Based Features </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_data[\"week_day\"]=combine_data.DATE.apply(lambda x:x.dayofweek)\n",
    "combine_data[\"time_day\"]=combine_data.DATE.apply(lambda x:x.hour)\n",
    "combine_data[\"month\"]=combine_data.DATE.apply(lambda x:x.month)\n",
    "combine_data[\"year\"]=combine_data.DATE.apply(lambda x:x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>week_day</th>\n",
       "      <th>time_day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07 06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09 08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10 16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ferleck ferles</td>\n",
       "      <td>2013-11-27 21:39:24</td>\n",
       "      <td>Subscribe to my channel ﻿</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>2013-11-28 16:30:13</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AUTHOR                DATE  \\\n",
       "0        Julius NM 2013-11-07 06:20:48   \n",
       "1  ElNino Melendez 2013-11-09 08:28:43   \n",
       "2           GsMega 2013-11-10 16:05:38   \n",
       "3   ferleck ferles 2013-11-27 21:39:24   \n",
       "4      BeBe Burkey 2013-11-28 16:30:13   \n",
       "\n",
       "                                             CONTENT  week_day  time_day  \\\n",
       "0  Huh, anyway check out this you[tube] channel: ...       3.0       6.0   \n",
       "1   me shaking my sexy ass on my channel enjoy ^_^ ﻿       5.0       8.0   \n",
       "2            watch?v=vtaRGgvGtWQ   Check this out .﻿       6.0      16.0   \n",
       "3                          Subscribe to my channel ﻿       2.0      21.0   \n",
       "4  and u should.d check my channel and tell me wh...       3.0      16.0   \n",
       "\n",
       "   month    year  \n",
       "0   11.0  2013.0  \n",
       "1   11.0  2013.0  \n",
       "2   11.0  2013.0  \n",
       "3   11.0  2013.0  \n",
       "4   11.0  2013.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#F57C00\"> Impute value for time based Features </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combine_data.columns[-4:]:\n",
    "    combine_data.fillna(value={col:99},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>week_day</th>\n",
       "      <th>time_day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ferleck ferles</td>\n",
       "      <td>Subscribe to my channel ﻿</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AUTHOR                                            CONTENT  \\\n",
       "0        Julius NM  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  ElNino Melendez   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "2           GsMega            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "3   ferleck ferles                          Subscribe to my channel ﻿   \n",
       "4      BeBe Burkey  and u should.d check my channel and tell me wh...   \n",
       "\n",
       "   week_day  time_day  month    year  \n",
       "0       3.0       6.0   11.0  2013.0  \n",
       "1       5.0       8.0   11.0  2013.0  \n",
       "2       6.0      16.0   11.0  2013.0  \n",
       "3       2.0      21.0   11.0  2013.0  \n",
       "4       3.0      16.0   11.0  2013.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data.drop(\"DATE\",axis=1,inplace=True)\n",
    "combine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>week_day</th>\n",
       "      <th>time_day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>traffic_group</th>\n",
       "      <th>day_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ferleck ferles</td>\n",
       "      <td>Subscribe to my channel ﻿</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AUTHOR                                            CONTENT  \\\n",
       "0        Julius NM  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  ElNino Melendez   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "2           GsMega            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "3   ferleck ferles                          Subscribe to my channel ﻿   \n",
       "4      BeBe Burkey  and u should.d check my channel and tell me wh...   \n",
       "\n",
       "   week_day  time_day  month    year traffic_group day_group  \n",
       "0       3.0       6.0   11.0  2013.0        normal   weekday  \n",
       "1       5.0       8.0   11.0  2013.0        normal   weekend  \n",
       "2       6.0      16.0   11.0  2013.0        normal   weekend  \n",
       "3       2.0      21.0   11.0  2013.0        normal   weekday  \n",
       "4       3.0      16.0   11.0  2013.0        normal   weekday  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data[\"traffic_group\"]=combine_data.time_day.apply(time_group)\n",
    "combine_data[\"day_group\"]=combine_data.week_day.apply(days_group)\n",
    "combine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df=pd.get_dummies(combine_data[[\"traffic_group\",\"day_group\"]],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data[\"url_presence\"]=combine_data[\"CONTENT\"].apply(url_presence)\n",
    "combine_data[\"url_presence\"][:1157].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_data[\"cleaned_content\"]=combine_data.CONTENT.apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#F57C00\"> Creating Content Based Features </span>\n",
    "\n",
    "**********************************************************\n",
    "    a) Length of comment\n",
    "    b) Presence of external url in comment\n",
    "    c) tf-idf based N-grams(1,3) \n",
    "**********************************************************    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_data[\"len_review\"]=combine_data.cleaned_content.apply(lambda x:len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>week_day</th>\n",
       "      <th>time_day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>traffic_group</th>\n",
       "      <th>day_group</th>\n",
       "      <th>url_presence</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>len_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julius NM</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekday</td>\n",
       "      <td>0</td>\n",
       "      <td>huh anyway check out this you tube channel kob...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekend</td>\n",
       "      <td>0</td>\n",
       "      <td>shake sexi as channel enjoy</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GsMega</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekend</td>\n",
       "      <td>0</td>\n",
       "      <td>watch vtarggvgtwq check this out</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ferleck ferles</td>\n",
       "      <td>Subscribe to my channel ﻿</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekday</td>\n",
       "      <td>0</td>\n",
       "      <td>subscrib channel</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BeBe Burkey</td>\n",
       "      <td>and u should.d check my channel and tell me wh...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>weekday</td>\n",
       "      <td>0</td>\n",
       "      <td>and should check channel and tell what should ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AUTHOR                                            CONTENT  \\\n",
       "0        Julius NM  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  ElNino Melendez   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "2           GsMega            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "3   ferleck ferles                          Subscribe to my channel ﻿   \n",
       "4      BeBe Burkey  and u should.d check my channel and tell me wh...   \n",
       "\n",
       "   week_day  time_day  month    year traffic_group day_group  url_presence  \\\n",
       "0       3.0       6.0   11.0  2013.0        normal   weekday             0   \n",
       "1       5.0       8.0   11.0  2013.0        normal   weekend             0   \n",
       "2       6.0      16.0   11.0  2013.0        normal   weekend             0   \n",
       "3       2.0      21.0   11.0  2013.0        normal   weekday             0   \n",
       "4       3.0      16.0   11.0  2013.0        normal   weekday             0   \n",
       "\n",
       "                                     cleaned_content  len_review  \n",
       "0  huh anyway check out this you tube channel kob...           9  \n",
       "1                        shake sexi as channel enjoy           5  \n",
       "2                   watch vtarggvgtwq check this out           5  \n",
       "3                                   subscrib channel           2  \n",
       "4  and should check channel and tell what should ...           9  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_clf=TfidfVectorizer(ngram_range=(1,3),max_features=200,norm ='l2',stop_words=\"english\")\n",
    "tfidf_data=tf_clf.fit_transform(combine_data.cleaned_content).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df=pd.DataFrame(data=tfidf_data,columns=tf_clf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([tfidf_df,time_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157, 204)\n",
      "(799, 204)\n"
     ]
    }
   ],
   "source": [
    "train_df=final_df[:train_data.shape[0]]\n",
    "test_df=final_df[train_data.shape[0]:]\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.DataFrame(data=train_df,columns=tf_clf.get_feature_names())\n",
    "df=train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#D81B60\"> Feature Reduction Techniques </span>\n",
    "\n",
    "**********************************\n",
    "    a) nearzerovariance\n",
    "    b) multicollinerity \n",
    "    c) auc reduction\n",
    "**********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_red_features=auc_reduction(df,true_labels=true_labels,threshold=0.1)\n",
    "df=df[auc_red_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Variables having variance < 0.0 is 204 out of 204\n"
     ]
    }
   ],
   "source": [
    "threshold=0.0\n",
    "non_near_zero_vars=near_zero_var(df,threshold=threshold)\n",
    "print(\"Number of Variables having variance < %s is %s out of %s\"%(threshold,len(non_near_zero_vars),len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    }
   ],
   "source": [
    "correlated_features=find_correlation(df[non_near_zero_vars],thresh=0.95)\n",
    "final_features=list(set(non_near_zero_vars)-set(correlated_features))\n",
    "print(len(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "final_train_df=train_df[final_features]\n",
    "final_test_df=test_df[final_features]\n",
    "final_train_df[\"url_presence\"]=combine_data[\"url_presence\"][:train_data.shape[0]].values\n",
    "final_train_df[\"len_review\"]=combine_data[\"len_review\"][:train_data.shape[0]].values\n",
    "final_test_df[\"url_presence\"]=combine_data[\"url_presence\"][train_data.shape[0]:].values\n",
    "final_test_df[\"len_review\"]=combine_data[\"len_review\"][train_data.shape[0]:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157, 178)\n",
      "(799, 178)\n"
     ]
    }
   ],
   "source": [
    "print(final_train_df.shape)\n",
    "print(final_test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#D81B60\"> Split Data into Train and Validation  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,valid_x,train_y,valid_y=train_test_split(final_train_df,true_labels,test_size=0.31,random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(798, 178)\n",
      "(359, 178)\n",
      "(799, 178)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(valid_x.shape)\n",
    "print(final_test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#D81B60\"> Execute the Classifications Models </span>\n",
    "\n",
    "***********************************************************************\n",
    "    a) Random Forest \n",
    "    b) Extra Tree Classifier\n",
    "    c) Gradient Boosting Machine\n",
    "    d) Logistic Regression\n",
    "    e) Support Vector Machine\n",
    "    f) Vooting Classifier(soft)\n",
    "***********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model : ########################  Random_Forest  ############################### \n",
      "Validate Model : ########################  Random_Forest  ############################### \n",
      "\n",
      "Training Error : \n",
      "0.9876543209876543\n",
      "\n",
      "Validation Error : \n",
      "0.9667111552548264\n",
      "\n",
      "Training Confusion Matrix :\n",
      "[[393   0]\n",
      " [ 10 395]]\n",
      "\n",
      "Validation Confusion Matrix :\n",
      "[[175   3]\n",
      " [  9 172]]\n",
      "\n",
      "Top 10 features from Random_Forest model \n",
      "                       features  importance\n",
      "176                url_presence   10.826242\n",
      "37                     subscrib   10.585461\n",
      "129                       check   10.134094\n",
      "124  traffic_group_unknown_time    6.214973\n",
      "177                  len_review    5.913495\n",
      "68                         plea    4.489510\n",
      "90                      channel    4.121508\n",
      "63                       youtub    3.443827\n",
      "133                        song    1.884689\n",
      "42                         view    1.844715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0001, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=True, random_state=2018, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf=RandomForestClassifier(random_state=2018,criterion='entropy',min_impurity_decrease=10e-5,max_depth=25,n_estimators=100,oob_score=True,bootstrap=True)\n",
    "train_test_model(rf_clf,\"Random_Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model : ########################  Extra_Classifier  ############################### \n",
      "Validate Model : ########################  Extra_Classifier  ############################### \n",
      "\n",
      "Training Error : \n",
      "0.9814814814814814\n",
      "\n",
      "Validation Error : \n",
      "0.969473586194053\n",
      "\n",
      "Training Confusion Matrix :\n",
      "[[393   0]\n",
      " [ 15 390]]\n",
      "\n",
      "Validation Confusion Matrix :\n",
      "[[175   3]\n",
      " [  8 173]]\n",
      "\n",
      "Top 10 features from Extra_Classifier model \n",
      "                       features  importance\n",
      "176                url_presence   12.414360\n",
      "124  traffic_group_unknown_time    9.597071\n",
      "37                     subscrib    8.646090\n",
      "129                       check    7.630790\n",
      "68                         plea    4.193055\n",
      "63                       youtub    3.367772\n",
      "90                      channel    2.840946\n",
      "152                video youtub    2.324157\n",
      "177                  len_review    2.292125\n",
      "128                 check video    2.245106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "           max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0001, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=True, random_state=2018, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_clf=ExtraTreesClassifier(random_state=2018,criterion='entropy',min_impurity_decrease=10e-5,max_depth=25,n_estimators=100,oob_score=True,bootstrap=True)\n",
    "train_test_model(ex_clf,\"Extra_Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model : ########################  GBM  ############################### \n",
      "Validate Model : ########################  GBM  ############################### \n",
      "\n",
      "Training Error : \n",
      "0.9851851851851852\n",
      "\n",
      "Validation Error : \n",
      "0.9639487243156\n",
      "\n",
      "Training Confusion Matrix :\n",
      "[[393   0]\n",
      " [ 12 393]]\n",
      "\n",
      "Validation Confusion Matrix :\n",
      "[[175   3]\n",
      " [ 10 171]]\n",
      "\n",
      "Top 10 features from GBM model \n",
      "                       features  importance\n",
      "129                       check   12.833853\n",
      "37                     subscrib   11.868914\n",
      "176                url_presence   11.798324\n",
      "124  traffic_group_unknown_time    7.238259\n",
      "177                  len_review    5.268125\n",
      "68                         plea    5.106862\n",
      "90                      channel    4.056572\n",
      "63                       youtub    3.628187\n",
      "128                 check video    2.037276\n",
      "142                       money    1.888610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.001, loss='exponential', max_depth=15,\n",
       "              max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0001, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              presort='auto', random_state=21, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_clf=GradientBoostingClassifier(random_state=21,learning_rate=0.001,n_estimators=500,\n",
    "                                   min_impurity_decrease=10e-5,max_depth=15,max_features=\"sqrt\",loss=\"exponential\")\n",
    "train_test_model(gbm_clf,\"GBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model : ########################  logistic_regresssion  ############################### \n",
      "Validate Model : ########################  logistic_regresssion  ############################### \n",
      "\n",
      "Training Error : \n",
      "0.9812553011026293\n",
      "\n",
      "Validation Error : \n",
      "0.9609535042522812\n",
      "\n",
      "Training Confusion Matrix :\n",
      "[[387   6]\n",
      " [  9 396]]\n",
      "\n",
      "Validation Confusion Matrix :\n",
      "[[170   8]\n",
      " [  6 175]]\n",
      "\n",
      "Regression Coefficient from logistic_regresssion model \n",
      "               features  coefficient\n",
      "0         check channel     0.765776\n",
      "1                 thumb     4.857782\n",
      "2                   tri     0.607753\n",
      "3                  hate    -1.197718\n",
      "4                 world     2.395354\n",
      "5     day_group_weekday    -2.472228\n",
      "6            make money     1.553114\n",
      "7                  look     0.172171\n",
      "8                 peopl    -0.530261\n",
      "9                   boy    -0.189669\n",
      "10                watch    -0.698406\n",
      "11    day_group_weekend    -2.346100\n",
      "12               shuffl    -1.865070\n",
      "13                 danc     0.674412\n",
      "14                video    -2.213088\n",
      "15               realli     1.761137\n",
      "16                  com     4.222280\n",
      "17                right    -1.479268\n",
      "18                dream     1.223861\n",
      "19                 love    -1.638937\n",
      "20                perri    -1.243174\n",
      "21              million     0.409848\n",
      "22               listen    -2.082120\n",
      "23                heard    -0.505405\n",
      "24                remix     0.906418\n",
      "25                 vote     2.278983\n",
      "26                  hey     1.224126\n",
      "27                  guy     1.169405\n",
      "28        plea subscrib     1.854766\n",
      "29               websit     3.970309\n",
      "..                  ...          ...\n",
      "148               cover     3.154495\n",
      "149                home     0.075323\n",
      "150            com make     0.193210\n",
      "151                 hit    -4.911681\n",
      "152        video youtub     3.555477\n",
      "153              beauti    -2.260320\n",
      "154                 plz     3.289988\n",
      "155               sorri     1.674648\n",
      "156                 psi    -2.405903\n",
      "157                 omg    -1.759268\n",
      "158                 ani    -0.373194\n",
      "159                nice     0.517539\n",
      "160       subscrib like     2.028067\n",
      "161                read     0.378839\n",
      "162           work home     0.000026\n",
      "163                cool    -0.170400\n",
      "164                 fan    -2.258446\n",
      "165                  wa    -4.096195\n",
      "166  traffic_group_peak    -0.129050\n",
      "167              follow     3.997262\n",
      "168              singer    -1.996390\n",
      "169                waka     0.574667\n",
      "170                 wow    -0.193749\n",
      "171                 old     0.130468\n",
      "172                time     0.483181\n",
      "173          make month     0.015166\n",
      "174          plea check     1.086809\n",
      "175                know    -1.645456\n",
      "176        url_presence     7.211996\n",
      "177          len_review     0.078509\n",
      "\n",
      "[178 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=500,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=2018,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_clf=LogisticRegressionCV(random_state=2018,cv=5,tol=10e-5,max_iter=500)\n",
    "train_test_model(logistic_clf,\"logistic_regresssion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model : ########################  SVC  ############################### \n",
      "Validate Model : ########################  SVC  ############################### \n",
      "\n",
      "Training Error : \n",
      "0.9726133257939874\n",
      "\n",
      "Validation Error : \n",
      "0.9693804705444161\n",
      "\n",
      "Training Confusion Matrix :\n",
      "[[387   6]\n",
      " [ 16 389]]\n",
      "\n",
      "Validation Confusion Matrix :\n",
      "[[173   5]\n",
      " [  6 175]]\n",
      "\n",
      "Regression Coefficient from SVC model \n",
      "               features  coefficient\n",
      "0         check channel     0.000000\n",
      "1                 thumb     2.038494\n",
      "2                   tri     0.296287\n",
      "3                  hate    -0.041916\n",
      "4                 world     0.327264\n",
      "5     day_group_weekday    -0.777485\n",
      "6            make money     0.894954\n",
      "7                  look    -0.175253\n",
      "8                 peopl     0.444103\n",
      "9                   boy     0.000000\n",
      "10                watch     0.038415\n",
      "11    day_group_weekend    -0.739097\n",
      "12               shuffl    -0.525723\n",
      "13                 danc     0.366875\n",
      "14                video    -0.298314\n",
      "15               realli     0.227929\n",
      "16                  com     1.750586\n",
      "17                right    -0.564685\n",
      "18                dream     0.275754\n",
      "19                 love    -0.104411\n",
      "20                perri    -0.265001\n",
      "21              million    -0.149182\n",
      "22               listen    -0.428721\n",
      "23                heard    -0.056032\n",
      "24                remix     0.272515\n",
      "25                 vote     0.922701\n",
      "26                  hey     0.231391\n",
      "27                  guy     0.563829\n",
      "28        plea subscrib     0.586577\n",
      "29               websit     0.826347\n",
      "..                  ...          ...\n",
      "148               cover     1.130721\n",
      "149                home     0.000000\n",
      "150            com make     0.000000\n",
      "151                 hit    -0.552259\n",
      "152        video youtub     0.776292\n",
      "153              beauti    -0.605532\n",
      "154                 plz     1.610277\n",
      "155               sorri     0.026360\n",
      "156                 psi    -0.726646\n",
      "157                 omg    -0.218024\n",
      "158                 ani    -0.017363\n",
      "159                nice     0.080034\n",
      "160       subscrib like     0.780032\n",
      "161                read     0.040825\n",
      "162           work home     0.000000\n",
      "163                cool    -0.119281\n",
      "164                 fan    -0.709945\n",
      "165                  wa    -0.585549\n",
      "166  traffic_group_peak    -0.021111\n",
      "167              follow     1.669473\n",
      "168              singer    -0.654230\n",
      "169                waka     0.029895\n",
      "170                 wow     0.059807\n",
      "171                 old     0.014116\n",
      "172                time     0.029701\n",
      "173          make month     0.000000\n",
      "174          plea check     0.278167\n",
      "175                know    -0.528222\n",
      "176        url_presence     2.119308\n",
      "177          len_review     0.029938\n",
      "\n",
      "[178 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=2018, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf=SVC(random_state=2018,max_iter=-1,C=1.5,kernel=\"linear\",probability=True)\n",
    "train_test_model(svc_clf,\"SVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model : ########################  vooting_classifier  ############################### \n",
      "Validate Model : ########################  vooting_classifier  ############################### \n",
      "\n",
      "Training Error : \n",
      "0.9838752238243332\n",
      "\n",
      "Validation Error : \n",
      "0.9749053324228693\n",
      "\n",
      "Training Confusion Matrix :\n",
      "[[391   2]\n",
      " [ 11 394]]\n",
      "\n",
      "Validation Confusion Matrix :\n",
      "[[173   5]\n",
      " [  4 177]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0001, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            mi...',\n",
       "  max_iter=-1, probability=True, random_state=2018, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft',\n",
       "         weights=[0.9667111552548264, 0.969473586194053, 0.9667111552548264, 0.9609535042522812, 0.9693804705444161])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voot_clf = VotingClassifier(voting='soft',weights=[0.9667111552548264,0.969473586194053,0.9667111552548264,\n",
    "                                                   0.9609535042522812,0.9693804705444161],\n",
    "                            estimators=[('rf', rf_clf),('ex_clf',ex_clf), ('gbm', gbm_clf),\\\n",
    "                                        ('log_reg', logistic_clf), ('svc', svc_clf)])\n",
    "train_test_model(voot_clf,\"vooting_classifier\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
